# Metaverse Pack in cellstrathub


**You can check the deployed metaverse pack on www.CellstratHub.com

Metaverse are fictitious versions of the internet that are accessible through virtual reality and augmented reality headsets as a single, immersive virtual world. The lack of standard technical specification of metaverse has led to the use of proprietary technologies. The use of numerous technologies involved in creating a metaverse is included in this Pack.

The pack contains following projects:

1) Digital Twin 
2) Egocentric Perception
3) Avatar generator
4) Text to Image generator
5) Facebook Omnivore  

# Digital twin benchmark model: 
The Digital Twin Benchmark Model is an RDF benchmark model that creates a semantic knowledge graph model representing the Digital Twin of a factory line. It represents some unique challenges and requirements in scaling Digital Twins as explained in the ISWC 2022 paper 

# Egocentric perception:
In this work, it presents an efficient technique for extracting temporal motifs in temporal networks. Our method is based on the novel notion of egocentric temporal neighborhoods, namely multi-layer structures centered on an ego node. Each temporal layer of the structure consists of the first-order neighborhood of the ego node, and corresponding nodes in sequential layers are connected by an edge. The strength of this approach lies in the possibility of encoding these structures into a unique bit vector, thus bypassing the problem of graph isomorphism in searching for temporal motifs. 

# Avatar generator
Feature Engineering is a process of manually constructing features that suit that task at hand. Our current feature is a color histogram, counting the number of colors in each image. But we could also contruct features in a different way, for example coundting the number of unique colors in each image. Feature engineering is common in traditional ML, but in deep learning the emphasis is more on the learning of the model itself, rather than the learning of the features. Specifically, finding the right model architecture. Feature engineering isn't really used any more. In this new version, we'll create a new feature vector using:

- the number of "GitHub grey" pixels in the image
- the number of distinct colors in the image

# Text to Image generator
Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance.

# Omnivore 
This repository contains PyTorch pretrained models, inference examples for the following papers:
Omnivore A single vision model for many different visual modalities, CVPR 2022 
OmniMAE Single Model Masked Pretraining on Images and Videos 
OmniVision Our training pipeline supporting the multi-modal vision research.
